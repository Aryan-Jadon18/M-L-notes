# ðŸ“Œ Landmark Machine Learning Papers

A curated list of influential research papers that shaped modern machine learning and artificial intelligence.

---

## ðŸ”‘ Key Papers

- **Attention Is All You Need (2017)**  
  Introduced the Transformer architecture, revolutionizing NLP and deep learning.  
  [Google Scholar Link](https://scholar.google.com/scholar_lookup?title=Attention+Is+All+You+Need&author=Vaswani&publication_year=2017)

- **ImageNet Classification with Deep Convolutional Neural Networks (AlexNet, 2012)**  
  Sparked the deep learning revolution in computer vision.  
  [Google Scholar Link](https://scholar.google.com/scholar_lookup?title=ImageNet+Classification+with+Deep+Convolutional+Neural+Networks&author=Krizhevsky&publication_year=2012)

- **Generative Adversarial Nets (GANs, 2014)**  
  Introduced adversarial training, enabling realistic image generation.  
  [Google Scholar Link](https://scholar.google.com/scholar_lookup?title=Generative+Adversarial+Nets&author=Goodfellow&publication_year=2014)

- **BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)**  
  Established contextual embeddings and transfer learning in NLP.  
  [Google Scholar Link](https://scholar.google.com/scholar_lookup?title=BERT:+Pre-training+of+Deep+Bidirectional+Transformers+for+Language+Understanding&author=Devlin&publication_year=2018)

- **Deep Residual Learning for Image Recognition (ResNet, 2015)**  
  Enabled training of ultra-deep networks using residual connections.  
  [Google Scholar Link](https://scholar.google.com/scholar_lookup?title=Deep+Residual+Learning+for+Image+Recognition&author=He&publication_year=2015)

- **Playing Atari with Deep Reinforcement Learning (Deep Q-Networks, 2013)**  
  Showed deep RL could achieve human-level performance in games.  
  [Google Scholar Link](https://scholar.google.com/scholar_lookup?title=Playing+Atari+with+Deep+Reinforcement+Learning&author=Mnih&publication_year=2013)

- **Auto-Encoding Variational Bayes (VAE, 2013)**  
  Introduced variational autoencoders for generative modeling.  
  [Google Scholar Link](https://scholar.google.com/scholar_lookup?title=Auto-Encoding+Variational+Bayes&author=Kingma&publication_year=2013)

- **Distilling the Knowledge in a Neural Network (Knowledge Distillation, 2015)**  
  Pioneered model compression and teacher-student learning.  
  [Google Scholar Link](https://scholar.google.com/scholar_lookup?title=Distilling+the+Knowledge+in+a+Neural+Network&author=Hinton&publication_year=2015)

- **Sequence to Sequence Learning with Neural Networks (2014)**  
  Introduced seq2seq models, foundational for translation and text generation.  
  [Google Scholar Link](https://scholar.google.com/scholar_lookup?title=Sequence+to+Sequence+Learning+with+Neural+Networks&author=Sutskever&publication_year=2014)

- **Mastering the Game of Go with Deep Neural Networks and Tree Search (AlphaGo, 2016)**  
  Landmark in reinforcement learning and AI strategy.  
  [Google Scholar Link](https://scholar.google.com/scholar_lookup?title=Mastering+the+Game+of+Go+with+Deep+Neural+Networks+and+Tree+Search&author=Silver&publication_year=2016)

---

## ðŸš€ Emerging ML Innovations

- **Vision Transformers (ViT)**  
  Transformers, originally designed for NLP, successfully applied to computer vision tasks.  
  They outperform traditional CNNs in image classification.  

- **Generative Adversarial Networks with Transformers (TransGAN)**  
  Incorporates transformers to ensure pixel-level consistency in generated images.  
  Achieved state-of-the-art results in image generation quality.  

- **TimeSformer (Facebook AI)**  
  Uses transformers to recognize actions in video clips.  
  Interprets sequences of video frames similar to how transformers process text.  

- **Multimodal Transformers**  
  Trained on text and fine-tuned across diverse domains including mathematics, logic, and computer vision.  
  Expands transformer applications beyond language.  

- **Transformers in Bioinformatics (AlphaFold 2)**  
  DeepMindâ€™s AlphaFold 2 predicts 3D protein structures from amino acid sequences.  
  Major breakthrough for drug discovery and biological research.  

---

## ðŸ“– Notes
- The **landmark papers** form the foundation of modern ML.  
- The **emerging innovations** highlight how transformers are reshaping multiple domains: vision, video, multimodal learning, and bioinformatics.  
- Together, they showcase the **evolution of machine learning** from CNNs to transformer-driven architectures.
